{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aman_Pandey_NLP_HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDT8QcEXdpc-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Author: Aman Pandey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY16StRbd7AB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Data Discription:\n",
        "\n",
        "The train.csv has three columns: id, text, and Target. text column has Tweets and Target column has sentiments (-1,0,1). In the Target column, values of -1,1,0 correspond to negative, positive, and neutral sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7JpmnPSm6DS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "41d40962-a050-4d2d-d175-74e7da761487"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import spacy\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "#from TokenizationTest import TokenizationTest\n",
        "from happyfuntokenizing import Tokenizer as potts\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import linear_model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1FadyHMWBNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T84-Y9pXZRFd",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da9O3a8nHlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cea1f13f-63a1-4bab-ddf3-497ff9518f84"
      },
      "source": [
        "# spaCy lemmatization needs tagger but disable the rest\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
        "nlp.remove_pipe('ner')\n",
        "nlp.remove_pipe('parser')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f0306f7b708>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wId9bgCwnaGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_part_of_speech_tags(token):\n",
        "    \n",
        "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
        "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
        "\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    \n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "    \n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv-zb-fEncex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def whitespace_tokenizer(data):\n",
        "  return str.split(data)\n",
        "\n",
        "def potter_tokenizer(data):\n",
        "  tokenizer= potts()\n",
        "  return tokenizer.tokenize(data)\n",
        "\n",
        "def spacy_tokenizer(data):\n",
        "  spacy_tokens=nlp(data)\n",
        "  return [token.text for token in spacy_tokens]\n",
        "\n",
        "def spacy_lemmatizer(data):\n",
        "  spacy_tokens=nlp(data)\n",
        "  return [token.lemma_ for token in spacy_tokens]\n",
        "\n",
        "def nltk_treebank_tokenizer(data):\n",
        "  tokenizer= TreebankWordTokenizer()\n",
        "  return tokenizer.tokenize(data)\n",
        "\n",
        "def nltk_treebank_stemmer_tokenizer(data):\n",
        "  tokenizer= TreebankWordTokenizer()\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens= tokenizer.tokenize(data)\n",
        "  return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "def nltk_treebank_lemmatizer_tokenizer(data):\n",
        "  tokenizer= TreebankWordTokenizer()\n",
        "  lemmatizer = WordNetLemmatizer()  \n",
        "  tokens= tokenizer.tokenize(data)\n",
        "  return [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in tokens]\n",
        "\n",
        "def nltk_TweetTokenizer_stemmer(data):\n",
        "  tokenizer = TweetTokenizer()\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens= tokenizer.tokenize(data)\n",
        "  return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "def nltk_TweetTokenizer_SnowBallstemmer(data):\n",
        "  tokenizer = TweetTokenizer()\n",
        "  stemmer = SnowballStemmer(language='english')\n",
        "  tokens= tokenizer.tokenize(data)\n",
        "  return [stemmer.stem(token) for token in tokens] \n",
        "\n",
        "def potter_tokenizer_stemmer(data):\n",
        "  tokenizer= potts()\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens= tokenizer.tokenize(data)\n",
        "  return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "def potter_tokenizer_lemmatizer(data):\n",
        "  tokenizer= potts()\n",
        "  lemmatizer = WordNetLemmatizer()  \n",
        "  tokens= tokenizer.tokenize(data)\n",
        "  return [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in tokens]\n",
        "\n",
        "def nltk_TweetTokenizer_lemmatizer_tokenizer(data):\n",
        "  tokenizer = TweetTokenizer()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens= tokenizer.tokenize(data)\n",
        "  return [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in tokens] \n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIwuNJd-7O6R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5203ae0d-1739-4afc-df64-7c50b26e30df"
      },
      "source": [
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "dat = pd.read_csv('/content/train.csv', encoding='latin1')\n",
        "dat.head()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>text</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>@USAirways  ! THE WORST in customer service. @...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>@united call wait times are over 20 minutes an...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>@JetBlue what's up with the random delay on fl...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>@AmericanAir Good morning!  Wondering why my p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>@united UA 746. Pacific Rim and Date Night cut...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id                                               text  Target\n",
              "0   1  @USAirways  ! THE WORST in customer service. @...      -1\n",
              "1   2  @united call wait times are over 20 minutes an...      -1\n",
              "2   3  @JetBlue what's up with the random delay on fl...      -1\n",
              "3   4  @AmericanAir Good morning!  Wondering why my p...       0\n",
              "4   5  @united UA 746. Pacific Rim and Date Night cut...      -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vxdvFHJY_if",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bbd232b9-8a70-4e98-c5e1-a6afddd20bf9"
      },
      "source": [
        "print(dat['text'][35])\n",
        "dat['text']= dat.text.str.replace('\\n','')\n",
        "print(dat['text'][35])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@united\n",
            "You really know how to piss people off. Your Farelock option is fake!\n",
            "@unitedYou really know how to piss people off. Your Farelock option is fake!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeWHOTnaEngI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "14d77c1d-fd0b-4308-b109-7a93bcecd32f"
      },
      "source": [
        "dat['Target'].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    4566\n",
              " 0    1536\n",
              " 1    1218\n",
              "Name: Target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8p-Id17nfQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\tdef read_data(filename, tokenizer):\n",
        "\t\tcorpus=[]\n",
        "\t\tY=[]\n",
        "\t\tfor i in range(len(dat)):\n",
        "\t\t#id=dat['Id']\n",
        "\t\t\ttext=dat['text'][i]\n",
        "\t\t#print(cols)\n",
        "\t\t\tlabel= dat['Target'][i]\n",
        "\t\t\ttokens=' '.join(tokenizer(text))\n",
        "\t\t\tcorpus.append(tokens)\n",
        "\t\t\tY.append(label)\n",
        "\t\t\t\t\n",
        "\t\t\n",
        "\t\treturn corpus, Y"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjEBnu4R8eTG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8198546-8464-4031-eed9-4d7d43b661bb"
      },
      "source": [
        "dat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7319, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrM3Vol58V0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainFile =  dat.iloc[:5000,]\n",
        "testFile =  dat.iloc[5000:,]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZibqsgh9qEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb1dc733-01bc-4d2d-a45b-fe19591a40a9"
      },
      "source": [
        "trainFile.shape, testFile.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 3), (2320, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjFAkbt-0iQE",
        "colab_type": "text"
      },
      "source": [
        "# Basic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYVPUhR9whhx",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcToXFbwxKzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uprs_gmniBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\tdef evaluate_tokenizer1(tokenizer):\n",
        "\t\t\n",
        "\t\ttrain_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "\t\ttest_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "\t\t\n",
        "\t\tvectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "\t\tX_train = vectorizer.fit_transform(train_corpus)\n",
        "\t\tX_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "\t\t#le = preprocessing.LabelEncoder()\n",
        "\t\t#le.fit(train_labels)\n",
        "\n",
        "\t\tY_train=train_labels\n",
        "\t\tY_test=test_labels\n",
        "\t\tparam_lr = {'C': [1,2,3,10,100],'penalty':['l1', 'l2']}\n",
        "\n",
        "\t\tgrid_PLR = GridSearchCV(LogisticRegression(max_iter=10000, solver='liblinear'), param_lr, cv=5,\n",
        "                          return_train_score=True, scoring='f1_macro')\n",
        "\t\tgrid_PLR.fit(X_train, Y_train)\n",
        "\t\tprint('Tokenizer', tokenizer.__name__)\n",
        "\t\tprint('train score: ', grid_PLR.score(X_train, Y_train))\n",
        "\t\tprint('test score: ', grid_PLR.score(X_test, Y_test))\n",
        "\t\n",
        "\t\tprint(\"Best parameters: {}\".format(grid_PLR.best_params_))\n",
        "\t\tprint(\"Best cross-validation score: {:.4f}\".format(grid_PLR.best_score_))\n",
        "\t\t\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9SMAJ0ynteU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "de24ff62-1db9-48fd-89dd-ed7c5c4923f0"
      },
      "source": [
        "tokenizers= [whitespace_tokenizer,potter_tokenizer, spacy_tokenizer, nltk_treebank_tokenizer, nltk_treebank_stemmer_tokenizer, nltk_treebank_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer1(tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer whitespace_tokenizer\n",
            "train score:  0.9380055953936335\n",
            "test score:  0.9380055953936335\n",
            "Best parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7053\n",
            "None\n",
            "Tokenizer spacy_tokenizer\n",
            "train score:  0.9378440289976937\n",
            "test score:  0.9378440289976937\n",
            "Best parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7325\n",
            "None\n",
            "Tokenizer nltk_treebank_tokenizer\n",
            "train score:  0.9347389845873417\n",
            "test score:  0.9347389845873417\n",
            "Best parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7303\n",
            "None\n",
            "Tokenizer nltk_treebank_stemmer_tokenizer\n",
            "train score:  0.9311519334529113\n",
            "test score:  0.9311519334529113\n",
            "Best parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7448\n",
            "None\n",
            "Tokenizer nltk_treebank_lemmatizer_tokenizer\n",
            "train score:  0.9311880373268272\n",
            "test score:  0.9311880373268272\n",
            "Best parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7370\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoBmIYXbOhpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer1(tokenizer):\n",
        "  \n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "  param_lr = {'C': [1,2,3,10,100],'penalty':['l1', 'l2']}\n",
        "\n",
        "  grid_PLR = GridSearchCV(LogisticRegression(max_iter=10000, solver='liblinear'), param_lr, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_PLR.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_PLR.score(X_train, Y_train))\n",
        "  print('test score: ', grid_PLR.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_PLR.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_PLR.best_score_))\n",
        "\t\t\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVuaK3E5Om7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "313f453d-0c11-4fd4-b5d0-e2b0ee5d396e"
      },
      "source": [
        "tokenizers= [nltk_TweetTokenizer_stemmer, nltk_TweetTokenizer_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer1(tokenizer))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer nltk_TweetTokenizer_stemmer\n",
            "train score:  0.8568420023056212\n",
            "test score:  0.8568420023056212\n",
            "Best parameters: {'C': 1, 'penalty': 'l1'}\n",
            "Best cross-validation score: 0.7448\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_lemmatizer_tokenizer\n",
            "train score:  0.8541423078201452\n",
            "test score:  0.8541423078201452\n",
            "Best parameters: {'C': 1, 'penalty': 'l1'}\n",
            "Best cross-validation score: 0.7354\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqkxR8Q3Y-RT",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression with Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HEW_zTcUBBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer5(tokenizer):\n",
        "  \n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, stop_words=\"english\", lowercase=False, strip_accents=None, binary=False)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "  param_lr = {'C': [1,2,3,10,100],'penalty':['l1', 'l2']}\n",
        "\n",
        "  grid_PLR = GridSearchCV(LogisticRegression(max_iter=10000, solver='liblinear'), param_lr, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_PLR.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_PLR.score(X_train, Y_train))\n",
        "  print('test score: ', grid_PLR.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_PLR.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_PLR.best_score_))\n",
        "\t\t\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDOl5QipUB6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "4e18dbe4-7dbd-4acd-8c20-0ea3c44405a9"
      },
      "source": [
        "tokenizers= [nltk_treebank_lemmatizer_tokenizer, nltk_TweetTokenizer_stemmer, nltk_TweetTokenizer_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer5(tokenizer))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer nltk_treebank_lemmatizer_tokenizer\n",
            "train score:  0.9334895168644403\n",
            "test score:  0.9334895168644403\n",
            "Best parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7306\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_stemmer\n",
            "train score:  0.8585291591423031\n",
            "test score:  0.8585291591423031\n",
            "Best parameters: {'C': 1, 'penalty': 'l1'}\n",
            "Best cross-validation score: 0.7403\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_lemmatizer_tokenizer\n",
            "train score:  0.9365982825163225\n",
            "test score:  0.9365982825163225\n",
            "Best parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7342\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlYAqRtBYnDe",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression with TfidVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJQJ9QAYZ7kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer6(tokenizer):\n",
        "  \n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "  param_lr = {'C': [1,2,3,10,100],'penalty':['l1', 'l2']}\n",
        "\n",
        "  grid_PLR = GridSearchCV(LogisticRegression(max_iter=10000, solver='liblinear'), param_lr, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_PLR.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_PLR.score(X_train, Y_train))\n",
        "  print('test score: ', grid_PLR.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_PLR.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_PLR.best_score_))\n",
        "\t\t\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__hIf8huZ9ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "e260605c-2cc3-4733-b5c1-538266107a84"
      },
      "source": [
        "tokenizers= [potter_tokenizer, nltk_treebank_lemmatizer_tokenizer, nltk_TweetTokenizer_stemmer, nltk_TweetTokenizer_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer6(tokenizer))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer potter_tokenizer\n",
            "train score:  0.8622186954224386\n",
            "test score:  0.8622186954224386\n",
            "Best parameters: {'C': 3, 'penalty': 'l1'}\n",
            "Best cross-validation score: 0.7159\n",
            "None\n",
            "Tokenizer nltk_treebank_lemmatizer_tokenizer\n",
            "train score:  0.8603404304414314\n",
            "test score:  0.8603404304414314\n",
            "Best parameters: {'C': 3, 'penalty': 'l1'}\n",
            "Best cross-validation score: 0.7259\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_stemmer\n",
            "train score:  0.8219731340606312\n",
            "test score:  0.8219731340606312\n",
            "Best parameters: {'C': 2, 'penalty': 'l1'}\n",
            "Best cross-validation score: 0.7250\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_lemmatizer_tokenizer\n",
            "train score:  0.8609480756265535\n",
            "test score:  0.8609480756265535\n",
            "Best parameters: {'C': 3, 'penalty': 'l1'}\n",
            "Best cross-validation score: 0.7275\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41oo0-b9Y1H-",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression with Hashing Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31LLN4-edMsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer7(tokenizer):\n",
        "  \n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = HashingVectorizer()\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "  param_lr = {'C': [1,2,3,10,100],'penalty':['l1', 'l2']}\n",
        "\n",
        "  grid_PLR = GridSearchCV(LogisticRegression(max_iter=10000, solver='liblinear'), param_lr, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_PLR.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_PLR.score(X_train, Y_train))\n",
        "  print('test score: ', grid_PLR.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_PLR.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_PLR.best_score_))\n",
        "\t\t\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L1gMw0TdM0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "3d2e8479-2e48-4ce4-9c3c-f2bfa73f764f"
      },
      "source": [
        "tokenizers= [potter_tokenizer, nltk_treebank_lemmatizer_tokenizer, nltk_TweetTokenizer_stemmer, nltk_TweetTokenizer_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer7(tokenizer))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer potter_tokenizer\n",
            "train score:  0.9067264682667906\n",
            "test score:  0.9067264682667906\n",
            "Best parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7228\n",
            "None\n",
            "Tokenizer nltk_treebank_lemmatizer_tokenizer\n",
            "train score:  0.8967545109022157\n",
            "test score:  0.8967545109022157\n",
            "Best parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7255\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_stemmer\n",
            "train score:  0.8915249104884592\n",
            "test score:  0.8915249104884592\n",
            "Best parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7309\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_lemmatizer_tokenizer\n",
            "train score:  0.8973321161193947\n",
            "test score:  0.8973321161193947\n",
            "Best parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best cross-validation score: 0.7285\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIlCwDMuwsnf",
        "colab_type": "text"
      },
      "source": [
        "## Na√Øve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPOhHbYrwsDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18UsgQDy_Nrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\tdef evaluate_tokenizer2(tokenizer):\n",
        "\t\t\n",
        "\t\ttrain_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "\t\ttest_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "\t\t\n",
        "\t\tvectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "\t\tX_train = vectorizer.fit_transform(train_corpus)\n",
        "\t\tX_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "\t\t#le = preprocessing.LabelEncoder()\n",
        "\t\t#le.fit(train_labels)\n",
        "\n",
        "\t\tY_train=train_labels\n",
        "\t\tY_test=test_labels\n",
        "\t\tnaive_param = { 'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001) }\n",
        "\n",
        "\t\tnaive_grid = GridSearchCV(MultinomialNB(), naive_param, cv=5,\n",
        "                          return_train_score=True,n_jobs=-1, scoring='f1_macro')\n",
        "\t\tnaive_grid.fit(X_train, Y_train)\n",
        "\t\tprint('Tokenizer', tokenizer.__name__)\n",
        "\t\tprint('train score: ', naive_grid.score(X_train, Y_train))\n",
        "\t\tprint('test score: ', naive_grid.score(X_test, Y_test))\n",
        "\t\n",
        "\t\tprint(\"Best parameters: {}\".format(naive_grid.best_params_))\n",
        "\t\tprint(\"Best cross-validation score: {:.4f}\".format(naive_grid.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_xUB4Pj_y90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "fa1fbf02-98df-4914-c89d-3d1796c4e353"
      },
      "source": [
        "tokenizers= [whitespace_tokenizer,potter_tokenizer, spacy_tokenizer, nltk_treebank_tokenizer, nltk_treebank_stemmer_tokenizer, nltk_treebank_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer2(tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer whitespace_tokenizer\n",
            "train score:  0.9168729804038124\n",
            "test score:  0.9168729804038124\n",
            "Best parameters: {'alpha': 0.1}\n",
            "Best cross-validation score: 0.6707\n",
            "None\n",
            "Tokenizer spacy_tokenizer\n",
            "train score:  0.9180460050127951\n",
            "test score:  0.9180460050127951\n",
            "Best parameters: {'alpha': 0.1}\n",
            "Best cross-validation score: 0.6941\n",
            "None\n",
            "Tokenizer nltk_treebank_tokenizer\n",
            "train score:  0.9164870809910312\n",
            "test score:  0.9164870809910312\n",
            "Best parameters: {'alpha': 0.1}\n",
            "Best cross-validation score: 0.6888\n",
            "None\n",
            "Tokenizer nltk_treebank_stemmer_tokenizer\n",
            "train score:  0.913222868270705\n",
            "test score:  0.913222868270705\n",
            "Best parameters: {'alpha': 0.1}\n",
            "Best cross-validation score: 0.6985\n",
            "None\n",
            "Tokenizer nltk_treebank_lemmatizer_tokenizer\n",
            "train score:  0.9152943740038655\n",
            "test score:  0.9152943740038655\n",
            "Best parameters: {'alpha': 0.1}\n",
            "Best cross-validation score: 0.6861\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuu0RfnPwtZ5",
        "colab_type": "text"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcXkLCvIw0NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOvBNN-KcvfD",
        "colab_type": "text"
      },
      "source": [
        "### XGBoost with different Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1o40GTXARVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer3(tokenizer):\n",
        "\t\t\n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "\n",
        "  xgbc= XGBClassifier(random_state=42,early_stopping_rounds=2,objective= 'multi:softmax')\n",
        "\n",
        "  xgbc_param = {\n",
        "            'max_depth' : [6, 8],\n",
        "            'n_estimators' : [200,300,400],\n",
        "            'learning_rate' : [ 0.4, 0.6, 0.8],\n",
        "             'min_child_weight' : [1,3],\n",
        "              #'subsample':[0.6,0.7,0.8,0.9,1]\n",
        "            }\n",
        "\n",
        "  grid_xgbc = GridSearchCV(xgbc, xgbc_param, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_xgbc.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_xgbc.score(X_train, Y_train))\n",
        "  print('test score: ', grid_xgbc.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_xgbc.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_xgbc.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd80Y7G9ATky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "9888e7e1-4c9e-4486-f97a-590f2e106a92"
      },
      "source": [
        "tokenizers= [whitespace_tokenizer,potter_tokenizer, spacy_tokenizer, nltk_treebank_tokenizer, nltk_treebank_stemmer_tokenizer, nltk_treebank_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer3(tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer whitespace_tokenizer\n",
            "train score:  0.953841117157232\n",
            "test score:  0.953841117157232\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 150}\n",
            "Best cross-validation score: 0.6992\n",
            "None\n",
            "Tokenizer spacy_tokenizer\n",
            "train score:  0.8994174092347021\n",
            "test score:  0.8994174092347021\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 3, 'n_estimators': 150}\n",
            "Best cross-validation score: 0.7327\n",
            "None\n",
            "Tokenizer nltk_treebank_tokenizer\n",
            "train score:  0.9668800242432911\n",
            "test score:  0.9668800242432911\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200}\n",
            "Best cross-validation score: 0.7205\n",
            "None\n",
            "Tokenizer nltk_treebank_stemmer_tokenizer\n",
            "train score:  0.9698399232619833\n",
            "test score:  0.9698399232619833\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200}\n",
            "Best cross-validation score: 0.7435\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHlrxCTa5EZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer4(tokenizer):\n",
        "\t\t\n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "\n",
        "  xgbc= XGBClassifier(random_state=42,early_stopping_rounds=2,objective= 'multi:softmax')\n",
        "\n",
        "  xgbc_param = {\n",
        "            'max_depth' : [6, 8,10],\n",
        "            'n_estimators' : [150,200,300],\n",
        "            'learning_rate' : [.4, 0.6,0.7],\n",
        "             'min_child_weight' : [1],\n",
        "              #'subsample':[0.6,0.7,0.8,0.9,1]\n",
        "            }\n",
        "\n",
        "  grid_xgbc = GridSearchCV(xgbc, xgbc_param, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_xgbc.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_xgbc.score(X_train, Y_train))\n",
        "  print('test score: ', grid_xgbc.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_xgbc.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_xgbc.best_score_))"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdq4PJG45IN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "8a9c83bd-c91f-443b-cde0-9b21f8abb129"
      },
      "source": [
        "tokenizers= [whitespace_tokenizer,potter_tokenizer, spacy_tokenizer, nltk_treebank_tokenizer, nltk_treebank_stemmer_tokenizer, nltk_treebank_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer4(tokenizer))"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer whitespace_tokenizer\n",
            "train score:  0.9437502620021648\n",
            "test score:  0.9437502620021648\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7035\n",
            "None\n",
            "Tokenizer spacy_tokenizer\n",
            "train score:  0.9628180818649038\n",
            "test score:  0.9628180818649038\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7328\n",
            "None\n",
            "Tokenizer nltk_treebank_tokenizer\n",
            "train score:  0.9704953868693681\n",
            "test score:  0.9704953868693681\n",
            "Best parameters: {'learning_rate': 0.7, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200}\n",
            "Best cross-validation score: 0.7230\n",
            "None\n",
            "Tokenizer nltk_treebank_stemmer_tokenizer\n",
            "train score:  0.9698399232619833\n",
            "test score:  0.9698399232619833\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200}\n",
            "Best cross-validation score: 0.7435\n",
            "None\n",
            "Tokenizer nltk_treebank_lemmatizer_tokenizer\n",
            "train score:  0.9645575317413811\n",
            "test score:  0.9645575317413811\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7289\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZs7RNwihWhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer7(tokenizer):\n",
        "\t\t\n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "\n",
        "  xgbc= XGBClassifier(random_state=42,early_stopping_rounds=2,objective= 'multi:softmax')\n",
        "\n",
        "  xgbc_param = {\n",
        "            'max_depth' : [6, 8],\n",
        "            'n_estimators' : [200,300],\n",
        "            'learning_rate' : [.4, 0.6,0.7],\n",
        "             'min_child_weight' : [1],\n",
        "              #'subsample':[0.6,0.7,0.8,0.9,1]\n",
        "            }\n",
        "\n",
        "  grid_xgbc = GridSearchCV(xgbc, xgbc_param, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_xgbc.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_xgbc.score(X_train, Y_train))\n",
        "  print('test score: ', grid_xgbc.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_xgbc.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_xgbc.best_score_))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNZ82gsrhWpb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "0c930eee-89da-4e04-bfa5-8ab405eebee8"
      },
      "source": [
        "tokenizers= [potter_tokenizer, nltk_treebank_stemmer_tokenizer, nltk_treebank_lemmatizer_tokenizer,nltk_TweetTokenizer_stemmer, nltk_TweetTokenizer_lemmatizer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer7(tokenizer))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer potter_tokenizer\n",
            "train score:  0.9703904579654933\n",
            "test score:  0.9703904579654933\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200}\n",
            "Best cross-validation score: 0.7444\n",
            "None\n",
            "Tokenizer nltk_treebank_stemmer_tokenizer\n",
            "train score:  0.967061715770095\n",
            "test score:  0.967061715770095\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7413\n",
            "None\n",
            "Tokenizer nltk_treebank_lemmatizer_tokenizer\n",
            "train score:  0.9810408425908683\n",
            "test score:  0.9810408425908683\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7281\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_stemmer\n",
            "train score:  0.9731402420141827\n",
            "test score:  0.9731402420141827\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200}\n",
            "Best cross-validation score: 0.7482\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_lemmatizer_tokenizer\n",
            "train score:  0.9711849000971821\n",
            "test score:  0.9711849000971821\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7365\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCEbuulGnFDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer8(tokenizer):\n",
        "\t\t\n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "\n",
        "  xgbc= XGBClassifier(random_state=42,early_stopping_rounds=2,objective= 'multi:softmax')\n",
        "\n",
        "  xgbc_param = {\n",
        "            'max_depth' : [5,6],\n",
        "            'n_estimators' : [200],\n",
        "            'learning_rate' : [0.6],\n",
        "             'min_child_weight' : [1],\n",
        "              'subsample':[0.6,0.7]\n",
        "            }\n",
        "\n",
        "  grid_xgbc = GridSearchCV(xgbc, xgbc_param, cv=3,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_xgbc.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_xgbc.score(X_train, Y_train))\n",
        "  print('test score: ', grid_xgbc.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_xgbc.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_xgbc.best_score_))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpZyuRuPnJbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "aba55be6-b0aa-472c-8751-87080dacb137"
      },
      "source": [
        "tokenizers= [ nltk_TweetTokenizer_SnowBallstemmer,   nltk_treebank_stemmer_tokenizer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer8(tokenizer))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer nltk_TweetTokenizer_SnowBallstemmer\n",
            "train score:  0.9646802909027682\n",
            "test score:  0.9646802909027682\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.6}\n",
            "Best cross-validation score: 0.7310\n",
            "None\n",
            "Tokenizer nltk_treebank_stemmer_tokenizer\n",
            "train score:  0.9665972169539022\n",
            "test score:  0.9665972169539022\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.7}\n",
            "Best cross-validation score: 0.7223\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya9mz6ANwpb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer8(tokenizer):\n",
        "\t\t\n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "\n",
        "  xgbc= XGBClassifier(random_state=42,early_stopping_rounds=2,objective= 'multi:softmax')\n",
        "\n",
        "  xgbc_param = {\n",
        "            'max_depth' : [6, 8],\n",
        "            'n_estimators' : [150, 200,300],\n",
        "            'learning_rate' : [.4, 0.6],\n",
        "             'min_child_weight' : [1],\n",
        "              #'subsample':[0.6,0.7,0.8,0.9,1]\n",
        "            }\n",
        "\n",
        "  grid_xgbc = GridSearchCV(xgbc, xgbc_param, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_xgbc.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_xgbc.score(X_train, Y_train))\n",
        "  print('test score: ', grid_xgbc.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_xgbc.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_xgbc.best_score_))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCLlzjDewpnL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "48b85d5f-f726-4d3d-af74-b3890a1861e8"
      },
      "source": [
        "tokenizers= [potter_tokenizer_lemmatizer, potter_tokenizer_lemmatizer, nltk_TweetTokenizer_SnowBallstemmer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer8(tokenizer))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer potter_tokenizer_lemmatizer\n",
            "train score:  0.9724521611031923\n",
            "test score:  0.9724521611031923\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7474\n",
            "None\n",
            "Tokenizer potter_tokenizer_lemmatizer\n",
            "train score:  0.9724521611031923\n",
            "test score:  0.9724521611031923\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7474\n",
            "None\n",
            "Tokenizer nltk_TweetTokenizer_SnowBallstemmer\n",
            "train score:  0.9759346866283684\n",
            "test score:  0.9759346866283684\n",
            "Best parameters: {'learning_rate': 0.4, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 300}\n",
            "Best cross-validation score: 0.7476\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGdlyLb2bVfa",
        "colab_type": "text"
      },
      "source": [
        "### XGBoost with Ngram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxWQF6BNUAxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_tokenizer9(tokenizer):\n",
        "\t\t\n",
        "  train_corpus, train_labels=read_data(trainFile, tokenizer)\n",
        "  test_corpus, test_labels=read_data(testFile, tokenizer)\n",
        "  \n",
        "  vectorizer = CountVectorizer(max_features=10000, ngram_range=(2, 2), analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "  X_train = vectorizer.fit_transform(train_corpus)\n",
        "  X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "  #le = preprocessing.LabelEncoder()\n",
        "  #le.fit(train_labels)\n",
        "\n",
        "  Y_train=train_labels\n",
        "  Y_test=test_labels\n",
        "\n",
        "  xgbc= XGBClassifier(random_state=42,early_stopping_rounds=2,objective= 'multi:softmax')\n",
        "\n",
        "  xgbc_param = {\n",
        "            'max_depth' : [6, 8],\n",
        "            'n_estimators' : [200,300],\n",
        "            'learning_rate' : [.4, 0.6,0.7],\n",
        "             'min_child_weight' : [1],\n",
        "              #'subsample':[0.6,0.7,0.8,0.9,1]\n",
        "            }\n",
        "\n",
        "  grid_xgbc = GridSearchCV(xgbc, xgbc_param, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "  grid_xgbc.fit(X_train, Y_train)\n",
        "  print('Tokenizer', tokenizer.__name__)\n",
        "  print('train score: ', grid_xgbc.score(X_train, Y_train))\n",
        "  print('test score: ', grid_xgbc.score(X_test, Y_test))\n",
        "\n",
        "  print(\"Best parameters: {}\".format(grid_xgbc.best_params_))\n",
        "  print(\"Best cross-validation score: {:.4f}\".format(grid_xgbc.best_score_))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcAwQbgNUMWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "61c36f1f-3851-4e5b-983b-62413bc4474a"
      },
      "source": [
        "tokenizers= [nltk_TweetTokenizer_stemmer]\n",
        "for tokenizer in tokenizers:\n",
        "  print(evaluate_tokenizer9(tokenizer))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer nltk_TweetTokenizer_stemmer\n",
            "train score:  0.9731402420141827\n",
            "test score:  0.9731402420141827\n",
            "Best parameters: {'learning_rate': 0.6, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200}\n",
            "Best cross-validation score: 0.7482\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV2uhaqHwWEq",
        "colab_type": "text"
      },
      "source": [
        "# Output File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdJk7x_jwYXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data_test(filename, tokenizer):\n",
        "  corpus=[]\n",
        "    #Y=[]\n",
        "  k=0\n",
        "  for i in range(len(filename)):\n",
        "    #id=dat['Id']\n",
        "    text=filename['text'][i]\n",
        "    #print(cols)\n",
        "    #label= dat['Target'][i]\n",
        "    tokens=' '.join(tokenizer(text))\n",
        "    corpus.append(tokens)\n",
        "    #Y.append(label)\n",
        "    \n",
        " \n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-c8nwM0oeTa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c97d5323-57bf-4c2f-83da-d3649b45db45"
      },
      "source": [
        "test_data = pd.read_csv('/content/test.csv', encoding='latin1')\n",
        "test_data.head()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7322</td>\n",
              "      <td>@AmericanAir In car gng to DFW. Pulled over 1h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7323</td>\n",
              "      <td>@AmericanAir after all, the plane didn√Ç¬â√É¬õ√Ç¬™t ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7324</td>\n",
              "      <td>@SouthwestAir can't believe how many paying cu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7325</td>\n",
              "      <td>@USAirways I can legitimately say that I would...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7326</td>\n",
              "      <td>@AmericanAir still no response from AA. great ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                               text\n",
              "0  7322  @AmericanAir In car gng to DFW. Pulled over 1h...\n",
              "1  7323  @AmericanAir after all, the plane didn√Ç¬â√É¬õ√Ç¬™t ...\n",
              "2  7324  @SouthwestAir can't believe how many paying cu...\n",
              "3  7325  @USAirways I can legitimately say that I would...\n",
              "4  7326  @AmericanAir still no response from AA. great ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6wYfaKDojWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data['text']= test_data.text.str.replace('\\n','')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUvJRMUPoeWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "e883a85c-e8ba-458b-fa91-b1f7abcc25f9"
      },
      "source": [
        "tokenizer=nltk_TweetTokenizer_stemmer\n",
        "\n",
        "train_corpus, train_labels=read_data(dat, tokenizer)\n",
        "test_corpus=read_data_test(test_data, tokenizer)\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#le.fit(train_labels)\n",
        "xgbc= XGBClassifier(random_state=42,early_stopping_rounds=2,objective= 'multi:softmax')\n",
        "Y_train=train_labels\n",
        "#Y_test=le.transform(test_labels)\n",
        "xgbc_param = {\n",
        "            'max_depth' : [6],\n",
        "            'n_estimators' : [200],\n",
        "            'learning_rate' : [0.6],\n",
        "             'min_child_weight' : [1],\n",
        "              #'subsample':[0.6,0.7,0.8,0.9,1]\n",
        "            }\n",
        "\n",
        "grid_final = GridSearchCV(xgbc, xgbc_param, cv=5,\n",
        "                        return_train_score=True, scoring='f1_macro')\n",
        "\n",
        "grid_final.fit(X_train, Y_train)\n",
        "\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                                     colsample_bylevel=1, colsample_bynode=1,\n",
              "                                     colsample_bytree=1,\n",
              "                                     early_stopping_rounds=2, gamma=0,\n",
              "                                     learning_rate=0.1, max_delta_step=0,\n",
              "                                     max_depth=3, min_child_weight=1,\n",
              "                                     missing=None, n_estimators=100, n_jobs=1,\n",
              "                                     nthread=None, objective='multi:softmax',\n",
              "                                     random_state=42, reg_alpha=0, reg_lambda=1,\n",
              "                                     scale_pos_weight=1, seed=None, silent=None,\n",
              "                                     subsample=1, verbosity=1),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'learning_rate': [0.6], 'max_depth': [6],\n",
              "                         'min_child_weight': [1], 'n_estimators': [200]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
              "             scoring='f1_macro', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaCn7DliwaCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Preprocessing of validation data, get predictions\n",
        "test_data_labels = grid_final.predict(X_test)\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL4ChM3PoqlC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f6826d4-5779-4b51-f3d3-fb8ff83ac9dd"
      },
      "source": [
        "# Create predictions to be submitted!\n",
        "pd.DataFrame({'id':test_data.id, 'Target': test_data_labels}).to_csv('sample_submission.csv', index =False)  \n",
        "print(\"Done :D\") "
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done :D\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}